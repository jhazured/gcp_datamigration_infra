
# -------------------------------
# ðŸŸ¢ Builder stage
# -------------------------------
FROM python:3.10-slim-bullseye AS builder

# Build arguments for flexibility
ARG SPARK_VERSION=3.5.1
ARG HADOOP_VERSION=3
ARG JAVA_VERSION=11

# Install build dependencies and Java
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        openjdk-${JAVA_VERSION}-jre-headless \
        curl \
        ca-certificates \
        build-essential \
        && \
    rm -rf /var/lib/apt/lists/*

# Set Java environment
ENV JAVA_HOME=/usr/lib/jvm/java-${JAVA_VERSION}-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Install Spark
ENV SPARK_VERSION=${SPARK_VERSION}
ENV HADOOP_VERSION=${HADOOP_VERSION}
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH

RUN curl -sSL https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    | tar -xz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    # Remove unnecessary files to reduce size
    rm -rf ${SPARK_HOME}/examples ${SPARK_HOME}/data ${SPARK_HOME}/R ${SPARK_HOME}/yarn

# Install Python dependencies
WORKDIR /install
COPY requirements/prod.txt .
RUN pip install --upgrade pip setuptools wheel && \
    pip install --no-cache-dir --user -r prod.txt

# -------------------------------
# ðŸŸ¢ Final runtime stage
# -------------------------------
FROM python:3.10-slim-bullseye

# Build arguments
ARG JAVA_VERSION=11

# Install minimal runtime dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        openjdk-${JAVA_VERSION}-jre-headless \
        procps \
        && \
    apt-get autoremove -y && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Copy Java and Spark from builder
COPY --from=builder /usr/lib/jvm /usr/lib/jvm
COPY --from=builder /opt/spark /opt/spark
COPY --from=builder /root/.local /usr/local

# Set environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-${JAVA_VERSION}-openjdk-amd64 \
    SPARK_HOME=/opt/spark \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONPATH=/app:$PYTHONPATH
ENV PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH

# Create non-root user with proper setup
RUN groupadd -r etl_user && \
    useradd -r -g etl_user -d /home/etl_user -s /bin/bash etl_user && \
    mkdir -p /home/etl_user /app /app/logs /app/data && \
    chown -R etl_user:etl_user /home/etl_user /app

# Copy application code
WORKDIR /app
COPY --chown=etl_user:etl_user . .

# Make scripts executable
RUN chmod +x scripts/tasks.sh scripts/run_pytest.sh scripts/run_bash.sh

# Switch to non-root user
USER etl_user

# Create additional directories user might need
RUN mkdir -p /app/spark-warehouse /app/spark-events

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import pyspark; print('Spark OK')" || exit 1

# Expose Spark UI port
EXPOSE 4040

# Add signal handling for graceful shutdown
STOPSIGNAL SIGTERM

# Default command
CMD ["python", "framework/main.py"]